{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f7098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c5930ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_input = torch.tensor([2.,2.,2.])\n",
    "# G_param = torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "# G_out = (G_param*G_input).detach()# detach()返回一个指向同一内存的新变量，唯一的不同点是返回的这个东西的require_grad = false，反向传播到此处戛然而止\n",
    "# D_param = torch.tensor([2.,2.,2.],requires_grad=True)\n",
    "# D_out = (G_out * D_param).sum()\n",
    "# D_out.backward()\n",
    "# print(D_param.grad,G_param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b56266b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_input = torch.tensor([1.])\n",
    "G_param = torch.tensor([1.],requires_grad=True)\n",
    "optimizer_G = optim.Adam([G_param], lr = 0.0001)\n",
    "D_param = torch.tensor([2.],requires_grad=True)\n",
    "optimizer_D = optim.Adam([D_param], lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4617cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得出G的输出，并且经过某种损失函数更新G，此法不通，最终决定CE和adv分开训练\n",
    "# G_out = (G_param*G_input).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8234d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对D反传后 |  D的梯度： 1.0  |D的参数大小： 2.0  |G的梯度： None  |G的参数大小： 1.0\n",
      "对D梯度下降后 |  D的梯度： 1.0  |D的参数大小： 1.999899983406067  |G的梯度： None  |G的参数大小： 1.0\n",
      "对D梯度清空梯度后 |  D的梯度： 0.0  |D的参数大小： 1.999899983406067  |G的梯度： None  |G的参数大小： 1.0\n"
     ]
    }
   ],
   "source": [
    "# 得到G的输出\n",
    "G_out = (G_param*G_input)\n",
    "D_real = (G_out.detach() * D_param).sum()#D要尽可能的把输出最小化，所以直接对输出梯度下降\n",
    "\n",
    "#D先更新\n",
    "D_real.backward()\n",
    "print(\"对D反传后 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n",
    "optimizer_D.step() #G是把detach掉的东西给D的，无梯度，D的参数都有梯度\n",
    "print(\"对D梯度下降后 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n",
    "optimizer_D.zero_grad()\n",
    "print(\"对D梯度清空梯度后 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4011ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#出现了bug，再D梯度下降之后，对D的参数进行了inplace操作，无法传梯度回去了，所以我依次更新可以吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb90fb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对D梯 |  D的梯度： -1.0  |D的参数大小： 1.999899983406067  |G的梯度： tensor([-1.9999])  |G的参数大小： 1.0\n",
      "对D梯 |  D的梯度： -1.0  |D的参数大小： 1.999899983406067  |G的梯度： tensor([-1.9999])  |G的参数大小： 1.000100016593933\n",
      "对D梯度清空梯度后 |  D的梯度： 0.0  |D的参数大小： 1.999899983406067  |G的梯度： tensor([-1.9999])  |G的参数大小： 1.000100016593933\n",
      "对D梯度清空梯度后 |  D的梯度： 0.0  |D的参数大小： 1.999899983406067  |G的梯度： tensor([0.])  |G的参数大小： 1.000100016593933\n"
     ]
    }
   ],
   "source": [
    "#G再更新\n",
    "D_fake = -(G_out * D_param).sum()#G要尽可能把输出最大化，所以对输出的负值梯度下降\n",
    "D_fake.backward()\n",
    "print(\"对D梯 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n",
    "optimizer_G.step() #只有G有梯度\n",
    "print(\"对D梯 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n",
    "optimizer_D.zero_grad()\n",
    "print(\"对D梯度清空梯度后 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())\n",
    "optimizer_G.zero_grad()\n",
    "print(\"对D梯度清空梯度后 | \",\"D的梯度：\",D_param.grad.item(),\" |D的参数大小：\",D_param.item()\n",
    "      ,\" |G的梯度：\",G_param.grad,\" |G的参数大小：\",G_param.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd90bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
