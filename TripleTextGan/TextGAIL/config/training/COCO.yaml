training:
    entropy_lamda : 0
    mode : "IPOT" # "GAIL"
    random_seed: 1
    num_gpus_per_node: 1
    batch_size: 32 #32   #64 data_loader用来采样数据的batch_size，这同样是
    local_rank: 0
    re_allocate: False
    triple: True
    resume:
        resume: False
        resume_model: True
        resume_optimizer: True
        resume_scheduler: True
        resume_rng_state: True
    checkpointing:
        directory: "Checkpoints"
        steps_interval: 10 #10
        seconds_interval: -1
        num_checkpoints_to_keep: 1000
        keep_checkpoint_every_num_seconds: 86400
    logging:
        level: "INFO"
        steps_interval: -1 # disabled when negative
        seconds_interval: 2 # disabled when `steps_interval` is set
    optimization:
        fp16: False
        fp16_opt_level: O1
        optimizer_name: AdamW
        learning_rate: 1e-5 # 1e-5一开始
        gradient_accumulation_steps: 1
        weight_decay: 0.01
        max_gradient_norm: -1.0
        warmup:
            scheduler_name: WarmupLinear
            warmup_steps: 100
    validation:
        steps_interval: 1 # after every epoch or none # 10
    total_num:
        epochs: -1
        update_steps: 60 # disabled when total_num_epochs > 0