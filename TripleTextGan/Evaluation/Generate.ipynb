{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ebad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import hydra.experimental\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from omegaconf import DictConfig\n",
    "import math\n",
    "from torchfly.text.decode import TransformerDecoder\n",
    "from torchfly.common import set_random_seed, move_to_device\n",
    "from fast_bleu import BLEU, SelfBLEU\n",
    "from configure_dataloader import DataLoaderHandler\n",
    "from model import Generator, TextGAILModel\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel,  RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a63bb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb98220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.experimental.initialize()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.experimental.initialize(\"./config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d6b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hqh/.conda/envs/torch1.10/lib/python3.9/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive task/COCO.yaml in file:///home/hqh/Triple-Gan/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/hqh/.conda/envs/torch1.10/lib/python3.9/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive training/COCO.yaml in file:///home/hqh/Triple-Gan/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/hqh/.conda/envs/torch1.10/lib/python3.9/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive text_gail/COCO.yaml in file:///home/hqh/Triple-Gan/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/hqh/.conda/envs/torch1.10/lib/python3.9/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive model/roberta-tokenized-gpt2.yaml in file:///home/hqh/Triple-Gan/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/hqh/.conda/envs/torch1.10/lib/python3.9/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive decode/default.yaml in file:///home/hqh/Triple-Gan/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "config = hydra.experimental.compose(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3427bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': {'name': 'COCO', 'data_dir': '/home/hqh/Triple-Gan/StyleCaption', 'weights_path': 'model_state.pth'}, 'training': {'entropy_lamda': 0, 'mode': 'IPOT', 'random_seed': 1, 'num_gpus_per_node': 1, 'batch_size': 32, 'local_rank': 0, 're_allocate': False, 'triple': True, 'resume': {'resume': False, 'resume_model': True, 'resume_optimizer': True, 'resume_scheduler': True, 'resume_rng_state': True}, 'checkpointing': {'directory': 'Checkpoints', 'steps_interval': 10, 'seconds_interval': -1, 'num_checkpoints_to_keep': 1000, 'keep_checkpoint_every_num_seconds': 86400}, 'logging': {'level': 'INFO', 'steps_interval': -1, 'seconds_interval': 2}, 'optimization': {'fp16': False, 'fp16_opt_level': 'O1', 'optimizer_name': 'AdamW', 'learning_rate': 1e-05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'max_gradient_norm': -1.0, 'warmup': {'scheduler_name': 'WarmupLinear', 'warmup_steps': 100}}, 'validation': {'steps_interval': 1}, 'total_num': {'epochs': -1, 'update_steps': 500}}, 'text_gail': {'batch_size': 'None', 'ppo_buffer_size': 32, 'sample_batch_size': 64, 'ppo_mini_batch_size': 8, 'ppo_epoch': 1, 'ppo_epsilon': 0.1, 'mix_human_demo_init_ratio': 0, 'mix_human_demo_ratio_warmup_steps': 100, 'discriminator_pretrain_steps': 100, 'constant_human_demo_reward': True, 'recompute_log_probs': True, 'entropy_reg_number': 0.001, 'MLE_alpha': 10, 'MLE_size': 8}, 'model': {'initializer_range': 0.02, 'layer_norm_epsilon': 1e-05, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'n_positions': 1024, 'vocab_size': 50265, 'embd_pdrop': 0.0, 'resid_pdrop': 0.0, 'attn_pdrop': 0.0, 'output_attentions': False, 'output_hidden_states': False, 'output_past': True, 'pad_token_id': 1, 'name': 'roberta-tokenized-gpt2'}, 'decode': {'num_return_sequences': 1, 'max_steps': 50, 'do_sample': True, 'num_beams': 1, 'temperature': 0.9, 'top_k': -1, 'top_p': 0.9, 'length_penalty': 1.0, 'bos_token_ids': [0], 'eos_token_ids': [2], 'output_log_probs': True, 'early_stopping': False, 'repetition_penalty': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# for key in config.keys():\n",
    "#     config[key]=config[key]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "532a0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/hqh/Triple-Gan/TextGAIL/outputs/2022-06-01/21-09-43/Checkpointsbest_f1_bleu_model.pth'\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48219a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51718571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TextGAILModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b13c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config.decode)\n",
    "decoder.register_generator(model.generator.decoder.to(device))\n",
    "decoder.register_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d5d40499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(model_path,map_location = device)\n",
    "#model.load_state_dict(weights)\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "df8061e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "styles =['fact','romantic','funny']\n",
    "s = {}\n",
    "s[\"fact\"] = torch.tensor([1,0,0,0])\n",
    "s[\"romantic\"] = torch.tensor([0, 1, 0,0])\n",
    "s[\"funny\"] = torch.tensor([0, 0, 1,0])\n",
    "num = 256\n",
    "\n",
    "for style in styles:\n",
    "    f_write = open(style+f\"-GEN.txt\", \"w\")\n",
    "    generated = []\n",
    "#     for i in range (5):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"input_ids\"] = torch.zeros((num,1),dtype=int).to(device)\n",
    "#     past = torch.randn(size=(12, 2, num, 12, 1, 61))  # .to(self.device)  # 61=64-3\n",
    "#     temp = s[style].unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "#     classification = torch.tile(temp, (12, 2, num, 12, 1, 1))\n",
    "#     model_inputs['past'] = torch.cat((classification, past), dim=-1).to(device)\n",
    "    temp = s[style].unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "    model_inputs['past'] = torch.tile(temp, (12, 2, num, 12, 1, 16)).to(device)\n",
    "    print(len(model_inputs[\"input_ids\"]))\n",
    "    #input = move_to_device(torch.zeros((512,1),dtype=int), device)\n",
    "    results = decoder.generate(model_inputs=model_inputs, temperature=0.6)\n",
    "\n",
    "\n",
    "    for i in range(len(results[\"tokens\"])):\n",
    "        res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "        generated.append(res)\n",
    "    for gen in generated:\n",
    "        f_write.write(json.dumps(gen))\n",
    "        f_write.write(\"\\n\")\n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceedcbc",
   "metadata": {},
   "source": [
    "## 同时可以测试class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "97ddaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "classifier_model = model.classifier.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7509cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898451b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Simple Seq2Seq Dataset Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, fact_filename, romantic_filename,funny_filename, tokenizer, add_bos_token=True, add_eos_token=True):\n",
    "        data = []\n",
    "        if fact_filename is not None:\n",
    "            with open(fact_filename,'r') as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"fact\"})\n",
    "                    line = f.readline()\n",
    "        if romantic_filename is not None:\n",
    "            with open(romantic_filename,'r') as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"romantic\"})\n",
    "                    line = f.readline() \n",
    "                    \n",
    "        if funny_filename is not None:\n",
    "            with open(funny_filename,'r') as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"funny\"})\n",
    "                    line = f.readline()    \n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.add_bos_token = add_bos_token\n",
    "        self.add_eos_token = add_eos_token\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        target_token_ids = self.tokenizer.encode(item[\"target\"], add_special_tokens=False)\n",
    "\n",
    "        if self.add_bos_token:\n",
    "            target_token_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "\n",
    "        if self.add_eos_token:\n",
    "            target_token_ids.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "        item[\"target_token_ids\"] = torch.LongTensor(target_token_ids)\n",
    "        \n",
    "        if item[\"style\"]=='fact':\n",
    "            item[\"source_token_ids\"] = [1,0,0]\n",
    "        elif item[\"style\"]=='romantic':\n",
    "            item[\"source_token_ids\"] = [0, 1, 0]\n",
    "        elif item[\"style\"]=='funny':\n",
    "            item[\"source_token_ids\"] = [0, 0, 1]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        new_batch = {}\n",
    "        new_batch[\"source_token_ids\"] = torch.tensor([item[\"source_token_ids\"] for item in batch])\n",
    "        new_batch[\"target_token_ids\"] = pad_sequence(\n",
    "            [item[\"target_token_ids\"] for item in batch], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        new_batch[\"style\"] = [item[\"style\"] for item in batch]\n",
    "        return new_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1d0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"../StyleCaption/fact-test.txt\"\n",
    "romantic_filename = \"../StyleCaption/romantic-test.txt\"\n",
    "funny_filename = \"../StyleCaption/funny-test.txt\"\n",
    "test_dataset = Seq2SeqDataset(fact_filename,romantic_filename,funny_filename , tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71a3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"../StyleCaption/fact-val.txt\"\n",
    "romantic_filename = \"../StyleCaption/romantic-val.txt\"\n",
    "funny_filename = \"../StyleCaption/funny-val.txt\"\n",
    "valid_dataset = Seq2SeqDataset(fact_filename,romantic_filename,funny_filename , tokenizer)\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=valid_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "88f35fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 32.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在验证集上,正确个数：999,总个数：1500,准确率:0.6660000085830688,fact:0.6980000138282776,romantic:0.6499999761581421,funny:0.6499999761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上,正确个数：992,总个数：1500,准确率:0.6613333225250244,fact:0.7179999947547913,romantic:0.6599999666213989,funny:0.6060000061988831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    epoch_num = 0 \n",
    "    epoch_real= 0\n",
    "    fact_epoch=0\n",
    "    romantic_epoch = 0\n",
    "    funny_epoch=0\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        sequence = batch['target_token_ids'].to(device)\n",
    "        label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "        label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "        sequence_logits = classifier_model(sequence).logits\n",
    "        #打印\n",
    "        pred_true = torch.argmax(sequence_logits,dim=-1) == label\n",
    "        batch_real = ( pred_true).sum()\n",
    "        batch_num = len(label)\n",
    "        epoch_real += batch_real\n",
    "        epoch_num += len(label)\n",
    "        #分门别类\n",
    "        fact_epoch +=  pred_true[(label==0).nonzero().reshape(-1)].sum()\n",
    "        romantic_epoch +=  pred_true[(label==1).nonzero().reshape(-1)].sum()\n",
    "        funny_epoch +=  pred_true[(label==2).nonzero().reshape(-1)].sum()\n",
    "    acc_rate = epoch_real/epoch_num\n",
    "    print(\"在验证集上,正确个数：{},总个数：{},准确率:{},fact:{},romantic:{},funny:{}\"\n",
    "      .format(epoch_real,epoch_num,acc_rate,fact_epoch*3/epoch_num,romantic_epoch*3/epoch_num,funny_epoch*3/epoch_num))\n",
    "    \n",
    "    epoch_num = 0 \n",
    "    epoch_real= 0\n",
    "    fact_epoch=0\n",
    "    romantic_epoch = 0\n",
    "    funny_epoch=0\n",
    "    for batch in tqdm.tqdm(test_dataloader):\n",
    "        sequence = batch['target_token_ids'].to(device)\n",
    "        label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "        label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "        sequence_logits = classifier_model(sequence).logits\n",
    "        #打印\n",
    "        pred_true = torch.argmax(sequence_logits,dim=-1) == label\n",
    "        batch_real = ( pred_true).sum()\n",
    "        batch_num = len(label)\n",
    "        epoch_real += batch_real\n",
    "        epoch_num += len(label)\n",
    "        #分门别类\n",
    "        fact_epoch +=  pred_true[(label==0).nonzero().reshape(-1)].sum()\n",
    "        romantic_epoch +=  pred_true[(label==1).nonzero().reshape(-1)].sum()\n",
    "        funny_epoch +=  pred_true[(label==2).nonzero().reshape(-1)].sum()\n",
    "    acc_rate = epoch_real/epoch_num\n",
    "    print(\"在测试集上,正确个数：{},总个数：{},准确率:{},fact:{},romantic:{},funny:{}\"\n",
    "      .format(epoch_real,epoch_num,acc_rate,fact_epoch*3/epoch_num,romantic_epoch*3/epoch_num,funny_epoch*3/epoch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150abcf",
   "metadata": {},
   "source": [
    "## 训练好的分类器来评价风格控制准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e7c7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pth = \"/home/hqh/Triple-Gan/classify.pth\"\n",
    "device =\"cpu\"\n",
    "my_classifier_model = torch.load(class_pth, map_location=device)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa4ac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"./funny-GEN.txt\"\n",
    "romantic_filename = \"./romantic-GEN.txt\"\n",
    "funny_filename = \"./funny-GEN.txt\"\n",
    "gen_dataset = Seq2SeqDataset(fact_filename=fact_filename,romantic_filename=romantic_filename,funny_filename=funny_filename,tokenizer=tokenizer)\n",
    "gen_dataloader = DataLoader(\n",
    "            gen_dataset, batch_size=batch_size, shuffle=True, collate_fn=gen_dataset .collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8269c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:18<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在验证集上,正确个数：768,总个数：1536,准确率:0.5,fact:0.701171875,romantic:0.5625,funny:0.236328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device =\"cpu\"\n",
    "with torch.no_grad():\n",
    "    epoch_num = 0 \n",
    "    epoch_real= 0\n",
    "    fact_epoch=0\n",
    "    romantic_epoch = 0\n",
    "    funny_epoch=0\n",
    "    for batch in tqdm.tqdm(gen_dataloader):\n",
    "        sequence = batch['target_token_ids'].to(device)\n",
    "        label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "        label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "        sequence_logits = my_classifier_model(sequence).logits\n",
    "        #打印\n",
    "        pred_true = torch.argmax(sequence_logits,dim=-1) == label\n",
    "        batch_real = ( pred_true).sum()\n",
    "        batch_num = len(label)\n",
    "        epoch_real += batch_real\n",
    "        epoch_num += len(label)\n",
    "        #分门别类\n",
    "        fact_epoch +=  pred_true[(label==0).nonzero().reshape(-1)].sum()\n",
    "        romantic_epoch +=  pred_true[(label==1).nonzero().reshape(-1)].sum()\n",
    "        funny_epoch +=  pred_true[(label==2).nonzero().reshape(-1)].sum()\n",
    "    acc_rate = epoch_real/epoch_num\n",
    "    print(\"在验证集上,正确个数：{},总个数：{},准确率:{},fact:{},romantic:{},funny:{}\"\n",
    "      .format(epoch_real,epoch_num,acc_rate,fact_epoch*3/epoch_num,romantic_epoch*3/epoch_num,funny_epoch*3/epoch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31bbce",
   "metadata": {},
   "source": [
    "## BLEU vs Self-BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "529bbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_data():\n",
    "    data = []\n",
    "    with open('../StyleCaption/funny-test.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower())\n",
    "            line = f.readline()\n",
    "    with open('../StyleCaption/fact-test.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower())\n",
    "            line = f.readline()\n",
    "    \n",
    "    with open('../StyleCaption/romantic-test.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower())\n",
    "            line = f.readline()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b6f7f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    data = []\n",
    "    with open('./funny-GEN.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower().replace(\"\\\"\",\"\"))\n",
    "            line = f.readline()\n",
    "    with open('./fact-GEN.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower().replace(\"\\\"\",\"\"))\n",
    "            line = f.readline()\n",
    "    \n",
    "    with open('./romantic-GEN.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            data.append(line.replace('\\n', '').lower().replace(\"\\\"\",\"\"))\n",
    "            line = f.readline()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7564f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref =ref_data()\n",
    "gen = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2065ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [tokenizer.encode(item, add_special_tokens=False) for item in ref]\n",
    "bleu = BLEU(ref, {'4gram': (0.25, 0.25, 0.25, 0.25)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b7621aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = [tokenizer.encode(item.lower(), add_special_tokens=False) for item in gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9cc58320",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = np.mean(bleu.get_score(gen)['4gram'])\n",
    "self_bleu = SelfBLEU(gen, {'4gram': (0.25,0.25,0.25,0.25)})\n",
    "self_bleu_score = np.mean(self_bleu.get_score()['4gram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d77377a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6055083638795318\n"
     ]
    }
   ],
   "source": [
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a91f1f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095954778851316\n"
     ]
    }
   ],
   "source": [
    "print(self_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e896411",
   "metadata": {},
   "source": [
    "## 计算G-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3732d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=math.sqrt(bleu_score*acc_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "953ba652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：0.5，BELU：0.6055083638795318，self-BLEU：0.8095954778851316，G-score：0.550231036515015\n"
     ]
    }
   ],
   "source": [
    "print(\"准确率：{}，BELU：{}，self-BLEU：{}，G-score：{}\".format(acc_rate,bleu_score,self_bleu_score,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ca107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
