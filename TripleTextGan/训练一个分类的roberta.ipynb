{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d97380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel,  RobertaForSequenceClassification\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import  torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085327a3",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e2f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =\"cuda:1\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98217f21",
   "metadata": {},
   "source": [
    "## 数据读取部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01bfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Simple Seq2Seq Dataset Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, fact_filename, romantic_filename,funny_filename, tokenizer, add_bos_token=True, add_eos_token=True):\n",
    "        data = []\n",
    "        with open(fact_filename,'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"fact\"})\n",
    "                line = f.readline()\n",
    "\n",
    "        with open(romantic_filename,'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"romantic\"})\n",
    "                line = f.readline()        \n",
    "\n",
    "        with open(funny_filename,'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                data.append({\"source\":\"\",\"target\":line.replace('\\n',''),\"style\":\"funny\"})\n",
    "                line = f.readline()    \n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.add_bos_token = add_bos_token\n",
    "        self.add_eos_token = add_eos_token\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        target_token_ids = self.tokenizer.encode(item[\"target\"], add_special_tokens=False)\n",
    "\n",
    "        if self.add_bos_token:\n",
    "            target_token_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "\n",
    "        if self.add_eos_token:\n",
    "            target_token_ids.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "        item[\"target_token_ids\"] = torch.LongTensor(target_token_ids)\n",
    "        \n",
    "        if item[\"style\"]=='fact':\n",
    "            item[\"source_token_ids\"] = [1,0,0]\n",
    "        elif item[\"style\"]=='romantic':\n",
    "            item[\"source_token_ids\"] = [0, 1, 0]\n",
    "        elif item[\"style\"]=='funny':\n",
    "            item[\"source_token_ids\"] = [0, 0, 1]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        new_batch = {}\n",
    "        new_batch[\"source_token_ids\"] = torch.tensor([item[\"source_token_ids\"] for item in batch])\n",
    "        new_batch[\"target_token_ids\"] = pad_sequence(\n",
    "            [item[\"target_token_ids\"] for item in batch], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        #sample_batch_size = len(new_batch[\"target_token_ids\"])\n",
    "        #past = torch.randn(size=(12, 2, sample_batch_size, 12, 1, 61))  # .to(self.device)  # 61=64-3\n",
    "        #temp = new_batch[\"source_token_ids\"].unsqueeze(1).unsqueeze(2).unsqueeze(0).unsqueeze(0)\n",
    "        #classification = torch.tile(temp, (12, 2, 1, 12, 1, 1))\n",
    "        #new_batch[\"past\"] = torch.cat((classification, past), dim=-1)\n",
    "        new_batch[\"style\"] = [item[\"style\"] for item in batch]\n",
    "        return new_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11422f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e032eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"./StyleCaption/fact-train.txt\"\n",
    "romantic_filename = \"./StyleCaption/romantic-train.txt\"\n",
    "funny_filename = \"./StyleCaption/funny-train.txt\"\n",
    "train_dataset = Seq2SeqDataset(fact_filename,romantic_filename,funny_filename , tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca7ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"./StyleCaption/fact-val.txt\"\n",
    "romantic_filename = \"./StyleCaption/romantic-val.txt\"\n",
    "funny_filename = \"./StyleCaption/funny-val.txt\"\n",
    "valid_dataset = Seq2SeqDataset(fact_filename,romantic_filename,funny_filename , tokenizer)\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeefdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_filename = \"./StyleCaption/fact-test.txt\"\n",
    "romantic_filename = \"./StyleCaption/romantic-test.txt\"\n",
    "funny_filename = \"./StyleCaption/funny-test.txt\"\n",
    "test_dataset = Seq2SeqDataset(fact_filename,romantic_filename,funny_filename , tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e05b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b155f3",
   "metadata": {},
   "source": [
    "## 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fffeace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a783f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661e2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 20\n",
    "# num_train_optimization_steps = int(18000 / 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fead21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "# lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=200, t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922c5e7",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31bcde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:24<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1轮epoch训练时在训练集上,正确个数：10641,总个数：18000,准确率:0.5911666750907898,学习率: 1e-05,loss: 0.8383419513702393\n",
      "第1轮epoch后，在测试集上,正确个数：1121,总个数：1500,准确率:0.7473333477973938\n",
      "第1轮epoch后，在验证集上,正确个数：1122,总个数：1500,准确率:0.7479999661445618\n",
      "保存模型更新 classifytensor(0.7480, device='cuda:1').pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:24<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2轮epoch训练时在训练集上,正确个数：14839,总个数：18000,准确率:0.8243889212608337,学习率: 1e-05,loss: 0.4548899233341217\n",
      "第2轮epoch后，在测试集上,正确个数：1166,总个数：1500,准确率:0.7773333191871643\n",
      "第2轮epoch后，在验证集上,正确个数：1164,总个数：1500,准确率:0.7759999632835388\n",
      "保存模型更新 classifytensor(0.7760, device='cuda:1').pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3轮epoch训练时在训练集上,正确个数：15533,总个数：18000,准确率:0.862944483757019,学习率: 1e-05,loss: 0.3747616708278656\n",
      "第3轮epoch后，在测试集上,正确个数：1155,总个数：1500,准确率:0.7699999809265137\n",
      "第3轮epoch后，在验证集上,正确个数：1150,总个数：1500,准确率:0.7666666507720947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4轮epoch训练时在训练集上,正确个数：15999,总个数：18000,准确率:0.8888333439826965,学习率: 1e-05,loss: 0.31293031573295593\n",
      "第4轮epoch后，在测试集上,正确个数：1157,总个数：1500,准确率:0.7713333368301392\n",
      "第4轮epoch后，在验证集上,正确个数：1141,总个数：1500,准确率:0.7606666684150696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5轮epoch训练时在训练集上,正确个数：16377,总个数：18000,准确率:0.9098333716392517,学习率: 1e-05,loss: 0.26089006662368774\n",
      "第5轮epoch后，在测试集上,正确个数：1136,总个数：1500,准确率:0.7573333382606506\n",
      "第5轮epoch后，在验证集上,正确个数：1146,总个数：1500,准确率:0.7639999985694885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6轮epoch训练时在训练集上,正确个数：16716,总个数：18000,准确率:0.9286666512489319,学习率: 1e-05,loss: 0.21465080976486206\n",
      "第6轮epoch后，在测试集上,正确个数：1139,总个数：1500,准确率:0.7593333125114441\n",
      "第6轮epoch后，在验证集上,正确个数：1145,总个数：1500,准确率:0.7633333206176758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7轮epoch训练时在训练集上,正确个数：16994,总个数：18000,准确率:0.9441111087799072,学习率: 1e-05,loss: 0.17573648691177368\n",
      "第7轮epoch后，在测试集上,正确个数：1151,总个数：1500,准确率:0.7673333287239075\n",
      "第7轮epoch后，在验证集上,正确个数：1162,总个数：1500,准确率:0.7746666669845581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8轮epoch训练时在训练集上,正确个数：17170,总个数：18000,准确率:0.9538888931274414,学习率: 1e-05,loss: 0.14749595522880554\n",
      "第8轮epoch后，在测试集上,正确个数：1135,总个数：1500,准确率:0.7566666603088379\n",
      "第8轮epoch后，在验证集上,正确个数：1148,总个数：1500,准确率:0.765333354473114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9轮epoch训练时在训练集上,正确个数：17230,总个数：18000,准确率:0.9572222232818604,学习率: 1e-05,loss: 0.13504867255687714\n",
      "第9轮epoch后，在测试集上,正确个数：1135,总个数：1500,准确率:0.7566666603088379\n",
      "第9轮epoch后，在验证集上,正确个数：1149,总个数：1500,准确率:0.765999972820282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10轮epoch训练时在训练集上,正确个数：17282,总个数：18000,准确率:0.960111141204834,学习率: 1e-05,loss: 0.1267026662826538\n",
      "第10轮epoch后，在测试集上,正确个数：1128,总个数：1500,准确率:0.7519999742507935\n",
      "第10轮epoch后，在验证集上,正确个数：1139,总个数：1500,准确率:0.7593333125114441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第11轮epoch训练时在训练集上,正确个数：17399,总个数：18000,准确率:0.9666111469268799,学习率: 1e-05,loss: 0.10553901642560959\n",
      "第11轮epoch后，在测试集上,正确个数：1148,总个数：1500,准确率:0.765333354473114\n",
      "第11轮epoch后，在验证集上,正确个数：1154,总个数：1500,准确率:0.7693333029747009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第12轮epoch训练时在训练集上,正确个数：17434,总个数：18000,准确率:0.9685555696487427,学习率: 1e-05,loss: 0.10218369215726852\n",
      "第12轮epoch后，在测试集上,正确个数：1131,总个数：1500,准确率:0.7540000081062317\n",
      "第12轮epoch后，在验证集上,正确个数：1146,总个数：1500,准确率:0.7639999985694885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第13轮epoch训练时在训练集上,正确个数：17323,总个数：18000,准确率:0.9623888731002808,学习率: 1e-05,loss: 0.11436948925256729\n",
      "第13轮epoch后，在测试集上,正确个数：1101,总个数：1500,准确率:0.7339999675750732\n",
      "第13轮epoch后，在验证集上,正确个数：1104,总个数：1500,准确率:0.7360000014305115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第14轮epoch训练时在训练集上,正确个数：17475,总个数：18000,准确率:0.9708333611488342,学习率: 1e-05,loss: 0.09412442147731781\n",
      "第14轮epoch后，在测试集上,正确个数：1112,总个数：1500,准确率:0.7413333058357239\n",
      "第14轮epoch后，在验证集上,正确个数：1120,总个数：1500,准确率:0.746666669845581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第15轮epoch训练时在训练集上,正确个数：17478,总个数：18000,准确率:0.9710000157356262,学习率: 1e-05,loss: 0.09151894599199295\n",
      "第15轮epoch后，在测试集上,正确个数：1101,总个数：1500,准确率:0.7339999675750732\n",
      "第15轮epoch后，在验证集上,正确个数：1119,总个数：1500,准确率:0.7459999918937683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第16轮epoch训练时在训练集上,正确个数：17406,总个数：18000,准确率:0.9670000076293945,学习率: 1e-05,loss: 0.10009369999170303\n",
      "第16轮epoch后，在测试集上,正确个数：1104,总个数：1500,准确率:0.7360000014305115\n",
      "第16轮epoch后，在验证集上,正确个数：1114,总个数：1500,准确率:0.7426666617393494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第17轮epoch训练时在训练集上,正确个数：17428,总个数：18000,准确率:0.9682222604751587,学习率: 1e-05,loss: 0.09777120500802994\n",
      "第17轮epoch后，在测试集上,正确个数：1125,总个数：1500,准确率:0.75\n",
      "第17轮epoch后，在验证集上,正确个数：1143,总个数：1500,准确率:0.7619999647140503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第18轮epoch训练时在训练集上,正确个数：17390,总个数：18000,准确率:0.9661111235618591,学习率: 1e-05,loss: 0.10167549550533295\n",
      "第18轮epoch后，在测试集上,正确个数：1137,总个数：1500,准确率:0.7580000162124634\n",
      "第18轮epoch后，在验证集上,正确个数：1141,总个数：1500,准确率:0.7606666684150696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第19轮epoch训练时在训练集上,正确个数：17444,总个数：18000,准确率:0.9691111445426941,学习率: 1e-05,loss: 0.09461299329996109\n",
      "第19轮epoch后，在测试集上,正确个数：1096,总个数：1500,准确率:0.7306666374206543\n",
      "第19轮epoch后，在验证集上,正确个数：1131,总个数：1500,准确率:0.7540000081062317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:25<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第20轮epoch训练时在训练集上,正确个数：17316,总个数：18000,准确率:0.9620000123977661,学习率: 1e-05,loss: 0.11264044791460037\n",
      "第20轮epoch后，在测试集上,正确个数：1093,总个数：1500,准确率:0.7286666631698608\n",
      "第20轮epoch后，在验证集上,正确个数：1085,总个数：1500,准确率:0.7233332991600037\n"
     ]
    }
   ],
   "source": [
    "acc_max = 0\n",
    "for epoch in range(epoch_n):\n",
    "    epoch_num = 0 \n",
    "    epoch_real= 0\n",
    "    n=0\n",
    "    loss = 0\n",
    "    for batch in tqdm.tqdm(train_dataloader):\n",
    "    #for batch in train_dataloader:\n",
    "        sequence = batch['target_token_ids'].to(device)\n",
    "        label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "        label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "        \n",
    "        sequence_logits = model(sequence).logits\n",
    "        \n",
    "        sequence_cross_entropy_loss = F.cross_entropy(sequence_logits, label)\n",
    "        sequence_cross_entropy_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #打印\n",
    "        loss+=sequence_cross_entropy_loss\n",
    "        n+=1\n",
    "        batch_real = (torch.argmax(sequence_logits,dim=-1) == label).sum()\n",
    "        batch_num = len(label)\n",
    "        epoch_real += batch_real\n",
    "        epoch_num += len(label)\n",
    "#         if n ==1:\n",
    "#         从这一行才看出来\n",
    "#             print(sequence)\n",
    "#             print(F.softmax( sequence_logits,dim=-1))\n",
    "#             print(label)\n",
    "#             print(torch.argmax(sequence_logits,dim=-1))\n",
    "#             print(torch.argmax(sequence_logits,dim=-1) == label)\n",
    "#             print((torch.argmax(sequence_logits,dim=-1) == label).sum())\n",
    "#         #\n",
    "#         print(\"{}-epoch| {}-th batch,准确率:{},loss:{}\".format(epoch+1,n, batch_real/batch_num,sequence_cross_entropy_loss))\n",
    "    acc_rate = epoch_real/epoch_num\n",
    "    print(\"第{}轮epoch训练时在训练集上,正确个数：{},总个数：{},准确率:{},学习率: {},loss: {}\"\n",
    "          .format(epoch+1,epoch_real,epoch_num,acc_rate,optimizer.param_groups[0]['lr'],loss/n))\n",
    "    #测试和验证\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        epoch_num = 0 \n",
    "        epoch_real= 0\n",
    "        for batch in test_dataloader:\n",
    "            sequence = batch['target_token_ids'].to(device)\n",
    "            label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "            label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "            sequence_logits = model(sequence).logits\n",
    "            #打印\n",
    "            batch_real = (torch.argmax(sequence_logits,dim=-1) == label).sum()\n",
    "            batch_num = len(label)\n",
    "            epoch_real += batch_real\n",
    "            epoch_num += len(label)\n",
    "        acc_rate = epoch_real/epoch_num\n",
    "        print(\"第{}轮epoch后，在测试集上,正确个数：{},总个数：{},准确率:{}\"\n",
    "          .format(epoch+1,epoch_real,epoch_num,acc_rate))\n",
    "        \n",
    "        epoch_num = 0 \n",
    "        epoch_real= 0\n",
    "        for batch in valid_dataloader:\n",
    "            sequence = batch['target_token_ids'].to(device)\n",
    "            label_onehot = batch['source_token_ids'] # n*3,tensor\n",
    "            label = torch.argmax(label_onehot,dim=-1).to(device) # n tensor\n",
    "            sequence_logits = model(sequence).logits\n",
    "            #打印\n",
    "            batch_real = (torch.argmax(sequence_logits,dim=-1) == label).sum()\n",
    "            batch_num = len(label)\n",
    "            epoch_real += batch_real\n",
    "            epoch_num += len(label)\n",
    "        acc_rate = epoch_real/epoch_num\n",
    "        print(\"第{}轮epoch后，在验证集上,正确个数：{},总个数：{},准确率:{}\"\n",
    "          .format(epoch+1,epoch_real,epoch_num,acc_rate))\n",
    "        \n",
    "        if acc_rate>acc_max:\n",
    "            acc_max  = acc_rate\n",
    "            print(\"保存模型更新\",'classify'+str(acc_rate)+'.pth')\n",
    "            torch.save(model,'classify.pth')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381dfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
